{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Json to CSV files/joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_videos='/media/aruiz/data/videos_data/'\n",
    "filename1='general_videos_data2.json'\n",
    "filename2='general_videos_data.csv'\n",
    "file1=os.path.join(data_folder_videos,filename1)\n",
    "file2=os.path.join(data_folder_videos,filename2)\n",
    "file3= \"json_errors.txt\"\n",
    "\n",
    "fieldnames = ['id', 'publishedAt', 'title','description','tags','categoryId','liveBroadcast','channelId','channelTitle',\n",
    "              'favoriteCount','commentCount','viewCount','likeCount','dislikeCount','duration','definition', 'caption',\n",
    "              'topicIds', 'freebaseTopIds']\n",
    "output=open(file2, 'a')\n",
    "\n",
    "writer = csv.DictWriter(output, fieldnames=fieldnames)\n",
    "# writer.writeheader()\n",
    "i=0\n",
    "\n",
    "with open (file1,'r') as f:\n",
    "    for line in f:\n",
    "        i+=1\n",
    "        if i>0:\n",
    "            try:\n",
    "                clean_dict={}\n",
    "                json_dict=json.loads(line)\n",
    "                videos= json_dict.get(\"items\",[])\n",
    "                if (videos!=[]):\n",
    "                    clean_dict['id']= videos[0]['id']\n",
    "                    clean_dict['publishedAt']= videos[0]['snippet']['publishedAt']\n",
    "                    clean_dict['channelId']= videos[0]['snippet']['channelId']\n",
    "                    clean_dict['title']= videos[0]['snippet']['title']\n",
    "                    \n",
    "                    if 'channelTitle' in videos[0]['snippet']:\n",
    "                        clean_dict['channelTitle']= videos[0]['snippet']['channelTitle']\n",
    "                    \n",
    "                    if 'viewCount' in videos[0]['statistics']:\n",
    "                        clean_dict['viewCount']=  videos[0]['statistics']['viewCount']\n",
    "\n",
    "                    if 'description' in videos[0]['snippet']:\n",
    "                        clean_dict['description']= videos[0]['snippet']['description']\n",
    "\n",
    "                    if 'tags' in videos[0]['snippet']:\n",
    "                        clean_dict['tags']= videos[0]['snippet']['tags']\n",
    "\n",
    "                    if 'categoryId' in videos[0]['snippet']:\n",
    "                        clean_dict['categoryId']= videos[0]['snippet']['categoryId']\n",
    "\n",
    "                    if 'liveBroadcastContent' in videos[0]['snippet']:\n",
    "                        clean_dict['liveBroadcast']= videos[0]['snippet']['liveBroadcastContent']\n",
    "\n",
    "                    if 'favoriteCount' in videos[0]['statistics']:\n",
    "                        clean_dict['favoriteCount']= videos[0]['statistics']['favoriteCount']\n",
    "\n",
    "                    if 'commentCount' in videos[0]['statistics']:\n",
    "                        clean_dict['commentCount']= videos[0]['statistics']['commentCount']\n",
    "\n",
    "                    if 'duration' in videos[0]['contentDetails']:\n",
    "                        clean_dict['duration']= videos[0]['contentDetails']['duration']\n",
    "\n",
    "                    if 'definition' in videos[0]['contentDetails']:\n",
    "                        clean_dict['definition']= videos[0]['contentDetails']['definition']\n",
    "\n",
    "                    if 'caption' in videos[0]['contentDetails']:\n",
    "                        clean_dict['caption']= videos[0]['contentDetails']['caption']\n",
    "\n",
    "                    if 'topicDetails' in videos[0]:\n",
    "                        if 'topicIds' in videos[0]['topicDetails']:\n",
    "                            clean_dict['topicIds']= videos[0]['topicDetails']['topicIds']\n",
    "                        if 'relevantTopicIds' in videos[0]['topicDetails']:\n",
    "                            clean_dict['freebaseTopIds']= videos[0]['topicDetails']['relevantTopicIds']\n",
    "\n",
    "                    if 'likeCount' in videos[0]['statistics']:\n",
    "                        clean_dict['likeCount']= videos[0]['statistics']['likeCount']\n",
    "\n",
    "                    if 'dislikeCount' in videos[0]['statistics']:\n",
    "                        clean_dict['dislikeCount']= videos[0]['statistics']['dislikeCount']\n",
    "\n",
    "                writer.writerow(clean_dict)\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                errors=open(file3, 'a')\n",
    "                errors.write(str(i)+'\\n')\n",
    "                errors.close()\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total=9632501\n",
    "#total 2 = 4365722\n",
    "#total general = 13998223\n",
    "#no duplicates = 11752812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting lines in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11752812"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open (file4, 'r') as csvfile:\n",
    "    fileObj=csv.reader(csvfile)\n",
    "    num = sum(1 for row in fileObj)\n",
    "num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aruiz/.virtualenvs/LetsPlayData/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "total_g = 13998222\n",
    "fieldnames = ['id', 'publishedAt', 'title','description','tags','categoryId','liveBroadcast','channelId','channelTitle',\n",
    "              'favoriteCount','commentCount','viewCount','likeCount','dislikeCount','duration','definition', 'caption',\n",
    "              'topicIds', 'freebaseTopIds']\n",
    "data_folder_videos='/media/aruiz/data/videos_data/'\n",
    "filename2='general_videos_data.csv'\n",
    "file4=os.path.join(data_folder_videos,'general_videos_clean.csv')\n",
    "file2=os.path.join(data_folder_videos,filename2)\n",
    "# df=dd.read_csv(file2,sep =',', header=0,encoding='utf-16')\n",
    "# df.head()\n",
    "\n",
    "# --------------- divide csv in chunks ---------------------------------------------------\n",
    "df = pd.read_csv(file2, header=0, names=fieldnames, encoding='utf-8', chunksize=1000000)\n",
    "ids=set()\n",
    "j=0\n",
    "\n",
    "for chunk in df:\n",
    "    j+=1\n",
    "    chunk = chunk.drop_duplicates(['id'], keep='first')\n",
    "    chunk = chunk[~chunk['id'].isin(ids)]\n",
    "    ids.update(chunk['id'].values)\n",
    "    chunk.to_csv(file4, encoding='utf-8', index=False, chunksize=1000000, mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save each community videos in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_channels='/media/aruiz/data/videos_data/'\n",
    "file1=os.path.join(data_folder_channels,'general_videos_clean.csv')\n",
    "file2=os.path.join(data_folder_channels,'node_data_bc.csv')\n",
    "fieldnames = ['id', 'publishedAt', 'title','description','tags','categoryId','liveBroadcast','channelId','channelTitle',\n",
    "              'favoriteCount','commentCount','viewCount','likeCount','dislikeCount','duration','definition', 'caption',\n",
    "              'topicIds', 'freebaseTopIds']\n",
    "\n",
    "#community chosen\n",
    "community_num=[9,22,85,76,71,37,87]\n",
    "\n",
    "for ind in range (1,len(community_num)):\n",
    "\n",
    "    # create csv to save community videos\n",
    "    file3=os.path.join(data_folder_channels,'data_community_'+str(community_num[ind])+'.csv')\n",
    "\n",
    "    # Data frame node data from graph communities\n",
    "    df1=pd.read_csv(file2)\n",
    "    channel_com = df1.drop(df1[df1['modularity_class']!= community_num[ind]].index)\n",
    "    com_ids=set()\n",
    "    com_ids.update(channel_com['Id'].values)\n",
    "\n",
    "    # Data frame all videos \n",
    "    df2 = pd.read_csv(file1, header=0, names=fieldnames, encoding='utf-8', chunksize=1000000)\n",
    "\n",
    "    for chunk in df2:\n",
    "        chunk = chunk[chunk['channelId'].isin(com_ids)]\n",
    "        chunk.to_csv(file3, encoding='utf-8', index=False, chunksize=1000000, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
