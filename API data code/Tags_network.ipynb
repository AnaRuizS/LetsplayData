{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channels' tags network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shlex\n",
    "import json\n",
    "import csv\n",
    "import nltk\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag network using .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_network(fileName, quantile, g, limit):\n",
    "    j=0\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df=pd.read_csv(fileName)\n",
    "    df = df[df.views>quantile]\n",
    "    df=df.sort_values(by=['views'],ascending=False)\n",
    "\n",
    "    for n in range(0,len(df)):\n",
    "        all_keys=[] # all tag/keywords for each channel without repetition\n",
    "        try:\n",
    "            if(df.keywords[n]!=\"\"):\n",
    "\n",
    "                try:\n",
    "                    for kw in shlex.split(df.keywords[n]):\n",
    "                        kw_id = kw.lower().replace(\"'\", \"\").replace(\"´\", \"\").replace(\"’\", \"\")\n",
    "                        single_keys=nltk.word_tokenize(kw_id)\n",
    "                        for key in single_keys:\n",
    "                            key2=lemmatizer.lemmatize(key)\n",
    "                            if key2 not in all_keys:\n",
    "                                all_keys.append(key2)\n",
    "\n",
    "                except:\n",
    "                    for kw in df.keywords[n].split():\n",
    "                        kw_id = kw.lower().replace(\"'\", \"\").replace(\"´\", \"\").replace(\"’\", \"\")\n",
    "                        single_keys=nltk.word_tokenize(kw_id)\n",
    "                        for key in single_keys:\n",
    "                            key2=lemmatizer.lemmatize(key)\n",
    "                            if key2 not in all_keys:\n",
    "                                all_keys.append(key2)\n",
    "                \n",
    "                #remove english stopwords and add to graph                \n",
    "                for key3 in all_keys:\n",
    "                    if key3 in stopwords.words('english'):\n",
    "                        all_keys.remove(key3)\n",
    "                    else:\n",
    "                        if g.has_node(key3):\n",
    "                            g.node[key3]['weight']+=1\n",
    "                        else:\n",
    "                            g.add_node(key3, type='keyword', weight=1)\n",
    "                        \n",
    "                #add all edges (all keywords appearing together will be connected)\n",
    "                edges= itertools.combinations(all_keys,2)\n",
    "                \n",
    "                for edge in edges:\n",
    "                    #if the edge exist add +1 weight\n",
    "                    if g.has_edge(edge[0],edge[1]):\n",
    "                        g[edge[0]][edge[1]]['weight'] += 1\n",
    "                    else:\n",
    "                        g.add_edge(edge[0],edge[1], weight=1)            \n",
    "                \n",
    "#                 filter just first limited number of channels\n",
    "#                 j+=1\n",
    "#                 if (j==limit):\n",
    "#                     break\n",
    "        except:\n",
    "            continue       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get degree quantile to filter information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantile(q_edge, gf):\n",
    "    #find maximum and minimum degree \n",
    "    degrees = [val for (node, val) in gf.degree()]\n",
    "    ctr=collections.Counter(degrees)\n",
    "    #find third quartile in degree\n",
    "    df2 = pd.DataFrame.from_dict(ctr,orient='index').reset_index()\n",
    "    q=df2.quantile(q_edge)\n",
    "    return float(q.values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nodes(new_node, node_list, gf):\n",
    "    H =gf.copy()\n",
    "    gf.add_node(new_node, type='keyword') # Add the 'merged' node   \n",
    "\n",
    "    for n1,n2,data in H.edges(data=True):\n",
    "        # For all edges related to one of the nodes to merge,\n",
    "        # make an edge going to or coming from the `new gene`.\n",
    "        if n1 in node_list:\n",
    "            g.add_edge(new_node,n2)\n",
    "        elif n2 in node_list:\n",
    "            g.add_edge(n1,new_node)\n",
    "\n",
    "    for n in node_list: # remove the merged nodes\n",
    "        gf.remove_node(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network using file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames=['minecraft', 'overwatch','agario', 'LOL', 'callOD','fivenaf','pkgo','roblox', 'gta','happyW']\n",
    "data_folder_channels='/media/aruiz/data/channels_clean_data/'\n",
    "fileName2=os.path.join(data_folder_channels,'all_channels_clean.csv')\n",
    "g = nx.Graph()\n",
    "create_tag_network(fileName2, 42261, g, 400)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "H =g.copy()\n",
    "i=0\n",
    "for node,d in H.nodes(data=True):\n",
    "    try:\n",
    "        if (d['weight']<3):\n",
    "            g.remove_node(node)\n",
    "    except:\n",
    "        i+=1\n",
    "        continue\n",
    "#     if g.degree(node)< q:\n",
    "#         g.remove_node(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17605\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "print(len(g))\n",
    "q=get_quantile(0.75,g)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minecraft --- node merging\n",
    "merge_nodes('lets play',['let','play'],g)\n",
    "merge_nodes('gta V',['gta','v'],g)\n",
    "merge_nodes('call of duty',['call','duty'],g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(g, \"testL.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "H=g.copy()\n",
    "\n",
    "for node, d in H.nodes(data=True):\n",
    "    try:\n",
    "        if node in english_vocab:\n",
    "            g.remove_node(node)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv(fileName)\n",
    "w=0\n",
    "H=g.copy()\n",
    "for node, d in H.nodes(data=True):\n",
    "    try:\n",
    "#         if node in list_nodes:\n",
    "#             print(d['weight'])\n",
    "        if(d['weight']<2):\n",
    "            g.remove_node(node)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of tags 23373\n",
    "#17605\n",
    "# 25% correctly classified as english\n",
    "# 75% classified"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
